# Miniature Distributed Systems

Note:- This readme is still half-baked :(

## So WTF is this?

- Its a mini version of distributed computing systems. More specifically it is comprises of a Webapp, server and worker units. These units work togather to provide compute
power to the clients.

## Who is it targeted for ?

- What you need to know was this was a fun project that was architected by me (Tejas Udupa), and we found out where this could be implimented even though its also not the proper
use case for our product.
- At the moment its aimed at Hospitals that generate data from various devices but cannot afford to buy server's but have large data which if processed can help them analyze
the patient problems more quickly.

### So what is it that we are processing ?

- We are processing Log/data generated by ECG and various machines (btw i have not seen this data its just a use case so spare me).
- These machines generate a lots of records in a single file for every poll (These machines use polling to get data from sensors on patients body.) they may for example
may contain timestamp, heart rate, blood pressure, breath rate, blood composition parameters and these data could be logged every second.
- These are huge files even though the filesize to us humans looks small(in order of MBs), but good luck going through it :).
- So these files need processing to get charts and various analysis information (charts and stuff).
- Now this could be solved with a single python script but when there are multiple patients and multiple files per patient it becomes a mess on the local system.
- So move eveyrything to the cloud :). Thats where we come !
- We have a webapp accecible via browser (still WIP). The clients will login into their accounts managed by the administrators. Once logged in they can do set of opererations:-
   - Upload new data file
   - Delete old data file
   - Process uploaded data file
   - View analysis result of processed data
   - Delete analysis data

## So how does all this magic happen?

- As previosly mentioned it comprises of web app, server and worker units lets see how they communicate and work togather. Words used here may or may not be present in the
actual system I am trying to explain in as simple words as possible.

#### WebApp
- When the client uploads data file to the webapp it is pushed into a SQL Server DB (this database is subject to change in the near future. Thnx Microsoft) in the 'user data' table.
- The Database would create a record for this data with various parameters which are all set to NULL/not present/not applicable.
- The client then submits file for processing, now this is where the fun begins.
- The Database sets the file record's submitted for processing as true.
  
#### Server
- Server is polling the Database continuosly and checking the 'user data' table for any updates.
- The Server keeps a timestamp and checks against all the latest timestamps in the database to see any new records were submitted and checks their 'submitted for processing'
(Note this is also somewhat over-ridden cause this was initally a VTU project for my final year and many features had to be purged to meet deadlines.)
- The Server pulls all the names of the files to be processed and passes it to the `DataExtactor` which extracts the information from the file and creates a data structure
which will be used later for sending out the data.
- The `DataExtactor` schedules the Data to be sent out the worker nodes.
- The `SenderCore` checks for avialable Workers. It finds the ideal worker for sending out this data for processing.
- The Selected `Worker` is queued with this data and when the assigned worker responds by locking a websocket coonection the `Worker` is sent this data in form of a `json`
string.
- The Server polls the worker for a acknowledgement of sent data and if not received it pushes the data again and waits.

#### Worker
- The worker units are also known as volenteer units as they are hosted by users who have volenteered to donate a part of their PC for processing. The volenteer host has the
worker program running on thier system. Its configured to use a certain amount of resources on the host system therfore the program doesn't overrun its resource allocated.
- Worker has the websocket always running and when it receives the data from the server it passes it the `Receiver` unit and pre-processes the packet in form of `json` string
if the packet is corrupted the packet is dropped and ERROR is sent out if partially corrupted else abandonded.
- If packet is required format then the acknowledgement is sent out to the server.
- The Received data is analysed by the `Receiver` and checked weather it is a `User data` or `Template` (This template is a format the user data needs follow and rules for
processing this data). And if found to be a User data (one sent by client in webapp) then it extracts feilds from `json` and sends it to `DataProcessor` unit.
- The `DataProcessor` checks validity of data against the rules(if against rules ERROR is sent out to server) and deletes duplicate data for ease of processing.
- The `DataProcessor` once done with its job it pushes it to the final unit `Algorithm`.
- `Algorithm` unit is the final unit. Every user data has a algorithm specification. The `Algorithm` module checks this and uses the correct the correct algoirthm for
processing the data. The final processed result is generated and sent out to the `Sender` unit.
- The `Sender` unit creates a packet and sends it out to the Server.
- Sever updates this in the Database with result and cleaned data from worker.

- `Receiver`,  `DataProcessor` and `Algorithm` from a pipeline also known as User data pipeline. These units run one after another.
- Note:- Worker has an internal scheduler to manage `Receiver`,  `DataProcessor` and `Algorithm` units, they are not continuosly executed rather time slotted and executed.
Each unit gets a slice of the CPU time and re-queued if they overrun their time slot, This allows other processes/pipelines to have a chance at using the CPU so large data
don't hog the CPU and Higher proiority tasks get completed quicker.

- Server and Worker has its own Docs please refer them for more information (Even these are WIP and are subject to change when new changes are introduced as its still in its
early stages.)

